{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a38751",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67ee863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from main and experiments library\n",
    "import os\n",
    "from experiments_lib import *\n",
    "os.chdir(\"../\")\n",
    "from library import *\n",
    "\n",
    "# filter the warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c8513",
   "metadata": {},
   "source": [
    "#### business failure prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c92de2",
   "metadata": {},
   "source": [
    "We use the ECL benchmark dataset to predict next-year business failure from the multi-modal data contained in corporate 10K records. To this end, we use the following variables:\n",
    "\n",
    "- ```qualified```: \"Yes\" if the 10K record qualifies for inclusion in the LoPucki BRD, \"No\" if the 10K record does not qualify for inclusion in the LoPucki BRD and \"out-of-period\" if the 10K records was filed before 1993 or after 2021.\n",
    "- ```can_label```: \"True\" if we have all the necessary information to assign a label to the 10K record (```filing date``` and ```total asset value```), \"False\" otherwise\n",
    "- ```label```: \"True\" if the company filed for bankruptcy in the year following the filing date of the 10K, \"False\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec589",
   "metadata": {},
   "source": [
    "#### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6336077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 115373 rows from CompuStat based on screening variables\n",
      "0 records in the dataset do not have an accompanying CompuStat record.\n"
     ]
    }
   ],
   "source": [
    "# read data and add financial features\n",
    "dataset = pd.read_csv('ECL.csv', index_col=0)\n",
    "dataset = compustat_local('./data/CompuStat/data.csv', dataset, update=False)\n",
    "dataset, predictors = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babcbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test set\n",
    "subset = dataset.loc[(dataset['can_label'] == True) & (dataset['qualified'] == 'Yes')].reset_index(drop=True)\n",
    "train = subset.loc[subset['bankruptcy_prediction_split'] == 'train']\n",
    "test = subset.loc[subset['bankruptcy_prediction_split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d15ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictors and labels\n",
    "X = train[predictors]\n",
    "y = train['label']\n",
    "\n",
    "test_X = test[predictors]\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8b2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample training data and store in a dictionary\n",
    "training_data = dict()\n",
    "\n",
    "# run over data distributions\n",
    "training_data['real'] = (X, y)\n",
    "for i in [1, 0.5, 0.25]:\n",
    "    \n",
    "    # resample and store\n",
    "    ros = RandomOverSampler(random_state=0, sampling_strategy=i)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    training_data[i] = (X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1efe3e",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1b4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RESULTS --\n",
      "AUC: 0.9148\n",
      "AP: 0.115\n",
      "recall@100: 0.1475\n",
      "CAP: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "distribution = 1\n",
    "\n",
    "LR = Pipeline([('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')), \n",
    "                 ('scaler', StandardScaler()), \n",
    "                 ('clf', LogisticRegression(penalty='l2', C = 0.01))])\n",
    "\n",
    "# train model\n",
    "LR.fit(X=training_data[distribution][0], y=training_data[distribution][1])\n",
    "\n",
    "# evaluate the model\n",
    "preds = LR.predict_proba(test_X)[:, 1]\n",
    "evaluate(labels=test_y, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb841c",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcd669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RESULTS --\n",
      "AUC: 0.9282\n",
      "AP: 0.1801\n",
      "recall@100: 0.2213\n",
      "CAP: 0.8564\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "distribution = 0.5\n",
    "\n",
    "# note that the output of the MLP depends on the initial weights - the results will slightly vary each run\n",
    "MLP = Pipeline([('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')), \n",
    "                         ('scaler', StandardScaler()), \n",
    "                         ('clf', MLPClassifier(learning_rate='invscaling', alpha=1, learning_rate_init=0.001,\n",
    "                                              hidden_layer_sizes=(100,100)))])\n",
    "# train model\n",
    "MLP.fit(X=training_data[distribution][0], y=training_data[distribution][1])\n",
    "\n",
    "# evaluate the model\n",
    "preds = MLP.predict_proba(test_X)[:, 1]\n",
    "evaluate(labels=test_y, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04b956",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8efc7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RESULTS --\n",
      "AUC: 0.9364\n",
      "AP: 0.1562\n",
      "recall@100: 0.1885\n",
      "CAP: 0.8727\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "distribution = 1\n",
    "\n",
    "XGB = Pipeline([ ('scaler', StandardScaler()), \n",
    "                 ('clf', xgb.XGBClassifier(objective='binary:logistic', subsample=0.5, eta=0.1, \n",
    "                 max_depth = 1, n_estimators = 1000))])\n",
    "\n",
    "# train model\n",
    "XGB.fit(X=training_data[distribution][0], y=training_data[distribution][1])\n",
    "\n",
    "# evaluate the model\n",
    "preds = XGB.predict_proba(test_X)[:, 1]\n",
    "evaluate(labels=test_y, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
