{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a5496c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import from main and experiments library\n",
    "import os\n",
    "from experiments_lib import *\n",
    "from roberta_lib import *\n",
    "os.chdir(\"../\")\n",
    "from library import *\n",
    "\n",
    "# filter the warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571421b",
   "metadata": {},
   "source": [
    "#### business failure prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f857102",
   "metadata": {},
   "source": [
    "We use the ECL benchmark dataset to predict next-year business failure from the multi-modal data contained in corporate 10K records. To this end, we use the following variables:\n",
    "\n",
    "- ```qualified```: \"Yes\" if the 10K record qualifies for inclusion in the LoPucki BRD, \"No\" if the 10K record does not qualify for inclusion in the LoPucki BRD and \"out-of-period\" if the 10K records was filed before 1993 or after 2021.\n",
    "- ```can_label```: \"True\" if we have all the necessary information to assign a label to the 10K record (```filing date``` and ```total asset value```), \"False\" otherwise\n",
    "- ```label```: \"True\" if the company filed for bankruptcy in the year following the filing date of the 10K, \"False\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1deacf",
   "metadata": {},
   "source": [
    "#### prepare data and pre-process text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4a90d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>company</th>\n",
       "      <th>period_of_report</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>datadate</th>\n",
       "      <th>filename</th>\n",
       "      <th>can_label</th>\n",
       "      <th>qualified</th>\n",
       "      <th>label</th>\n",
       "      <th>bankruptcy_prediction_split</th>\n",
       "      <th>bankruptcy_date_1</th>\n",
       "      <th>bankruptcy_date_2</th>\n",
       "      <th>bankruptcy_date_3</th>\n",
       "      <th>filing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27792</th>\n",
       "      <td>1308161.0</td>\n",
       "      <td>NEWS CORP</td>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>12886.0</td>\n",
       "      <td>30/06/2010</td>\n",
       "      <td>/2010/1308161_10K_2010_0001193125-10-181329.json</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>4828.0</td>\n",
       "      <td>AMERICAN CRYSTAL SUGAR CO /MN/</td>\n",
       "      <td>2000-08-31</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>31/08/2000</td>\n",
       "      <td>/2000/4828_10K405_2000_0000912057-00-051146.json</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40245</th>\n",
       "      <td>863436.0</td>\n",
       "      <td>BENCHMARK ELECTRONICS INC</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>23084.0</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>/2012/863436_10K_2012_0001144204-13-011827.json</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77007</th>\n",
       "      <td>1304280.0</td>\n",
       "      <td>NOVELIS INC.</td>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>162701.0</td>\n",
       "      <td>31/03/2008</td>\n",
       "      <td>/2008/1304280_10K_2008_0000950144-08-004924.json</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-06-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55953</th>\n",
       "      <td>874766.0</td>\n",
       "      <td>ITT HARTFORD GROUP INC /DE</td>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>61739.0</td>\n",
       "      <td>31/12/1996</td>\n",
       "      <td>/1996/874766_10K_1996_0000948572-97-000016.json</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-03-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cik                         company period_of_report     gvkey  \\\n",
       "27792  1308161.0                       NEWS CORP       2010-06-30   12886.0   \n",
       "914       4828.0  AMERICAN CRYSTAL SUGAR CO /MN/       2000-08-31    1429.0   \n",
       "40245   863436.0       BENCHMARK ELECTRONICS INC       2012-12-31   23084.0   \n",
       "77007  1304280.0                    NOVELIS INC.       2008-03-31  162701.0   \n",
       "55953   874766.0      ITT HARTFORD GROUP INC /DE       1996-12-31   61739.0   \n",
       "\n",
       "         datadate                                          filename  \\\n",
       "27792  30/06/2010  /2010/1308161_10K_2010_0001193125-10-181329.json   \n",
       "914    31/08/2000  /2000/4828_10K405_2000_0000912057-00-051146.json   \n",
       "40245  31/12/2012   /2012/863436_10K_2012_0001144204-13-011827.json   \n",
       "77007  31/03/2008  /2008/1304280_10K_2008_0000950144-08-004924.json   \n",
       "55953  31/12/1996   /1996/874766_10K_1996_0000948572-97-000016.json   \n",
       "\n",
       "       can_label qualified  label bankruptcy_prediction_split  \\\n",
       "27792       True       Yes  False                       train   \n",
       "914         True       Yes  False                       train   \n",
       "40245       True       Yes  False                       train   \n",
       "77007       True       Yes  False                       train   \n",
       "55953       True       Yes  False                       train   \n",
       "\n",
       "      bankruptcy_date_1 bankruptcy_date_2 bankruptcy_date_3 filing_date  \n",
       "27792               NaN               NaN               NaN  2010-08-06  \n",
       "914                 NaN               NaN               NaN  2000-11-22  \n",
       "40245               NaN               NaN               NaN  2013-02-28  \n",
       "77007               NaN               NaN               NaN  2008-06-19  \n",
       "55953               NaN               NaN               NaN  1997-03-28  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify path\n",
    "path_ECL = '../bankruptcy research data/ECL.csv' # change path to correct location\n",
    "\n",
    "# read data \n",
    "dataset = pd.read_csv(path_ECL, index_col=0)\n",
    "subset = dataset.loc[(dataset['can_label'] == True) & (dataset['qualified'] == 'Yes')].reset_index(drop=True)\n",
    "subset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b17f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpera already exist\n"
     ]
    }
   ],
   "source": [
    "# path to corpera\n",
    "original_corpus = '../bankruptcy research data/original_corpus' # change path to correct location\n",
    "clean_corpus = '../bankruptcy research data/clean_corpus'\n",
    "raw_corpus = '../bankruptcy research data/raw_corpus'\n",
    "\n",
    "# indicate of we still need to perform cleaning operations \n",
    "clean = True\n",
    "\n",
    "# Create directories\n",
    "try:\n",
    "    os.mkdir(clean_corpus + '/')\n",
    "    os.mkdir(raw_corpus + '/')\n",
    "    for i in range(1993,2024):\n",
    "        os.mkdir(clean_corpus + '/' + str(i))\n",
    "        os.mkdir(raw_corpus + '/' + str(i))\n",
    "except:\n",
    "    print('Corpera already exist')\n",
    "    clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524fea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# clean documents if indicated\n",
    "if clean:\n",
    "\n",
    "    # loop over documents\n",
    "    for idx, row in prediction_subset.iterrows():\n",
    "\n",
    "        # read file\n",
    "        filename = row['filename']\n",
    "        file_path = original_corpus + filename\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_data = json.load(f)\n",
    "\n",
    "        # extract relevant part and clean\n",
    "        document = file_data.get('item_7', '')\n",
    "        tokens = tokenize_lemmatize(document)\n",
    "        clean_tokens = remove_stop_punct_num(tokens)\n",
    "        clean_document = ' '.join(clean_tokens)\n",
    "\n",
    "        # create file paths\n",
    "        file_name_without_extension = os.path.splitext(filename)[0]\n",
    "        preprocessed_filepath = clean_corpus + file_name_without_extension + '.txt'\n",
    "        raw_filepath = raw_corpus + file_name_without_extension + '.txt'\n",
    "\n",
    "        # store\n",
    "        with open(preprocessed_filepath, \"w\", encoding=\"utf-8\") as preprocessed_file:\n",
    "            preprocessed_file.write(clean_document)\n",
    "\n",
    "        with open(raw_filepath, \"w\", encoding=\"utf-8\") as raw_file:\n",
    "            raw_file.write(document)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust file extension\n",
    "subset['filename'] = subset['filename'].str.replace('.json', '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a908ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "train = subset.loc[subset['bankruptcy_prediction_split'] == 'train']\n",
    "test = subset.loc[subset['bankruptcy_prediction_split'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1efe3e",
   "metadata": {},
   "source": [
    "## TF-IDF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc544895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split predictors and labels\n",
    "train_X = clean_corpus + train['filename']\n",
    "test_X = clean_corpus + test['filename']\n",
    "\n",
    "train_y = train['label']\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89052b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RESULTS --\n",
      "AUC: 0.8855\n",
      "AP: 0.2387\n",
      "recall@100: 0.2869\n",
      "CAP: 0.7711\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "TF_IDF = Pipeline([\n",
    "        ('vect', TfidfVectorizer(input='filename', lowercase=True, \n",
    "                                 strip_accents='ascii', stop_words='english', min_df=2, ngram_range = (1,2))),\n",
    "        ('clf', LogisticRegression(penalty = 'l1', C = 1, class_weight = 'balanced', \n",
    "                                   solver='liblinear'))])\n",
    "\n",
    "# train model\n",
    "TF_IDF.fit(X=train_X, y= train_y)\n",
    "\n",
    "# evaluate the model\n",
    "preds = TF_IDF.predict_proba(test_X)[:, 1]\n",
    "evaluate(labels=test_y, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf901673",
   "metadata": {},
   "source": [
    "## RoBERTa classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba92c8b",
   "metadata": {},
   "source": [
    "#### Initialize model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load pretrained RoBERTa tokenizer and model \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params - note that gradient accumulation is used to simulate larger batches\n",
    "batch_size = 16\n",
    "learning_rate = 2e-3\n",
    "num_epochs = 2\n",
    "accumulation_steps = 20\n",
    "\n",
    "# set optimiser and  weighted loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "class_counts = train['label'].value_counts().to_dict()\n",
    "total_samples = len(train)\n",
    "class_weights = [total_samples / (2 * class_counts[False]), total_samples / (2 * class_counts[True])]\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46fd7e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "train_dataset = CustomDataset(train, tokenizer, raw_corpus)\n",
    "test_dataset = CustomDataset(test, tokenizer, raw_corpus)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c12a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers in first epoch\n",
    "model.train()\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # unfreeze\n",
    "    if epoch == 1:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = True\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "    # loop over batches\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        # get inputs\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits \n",
    "\n",
    "        # backward pass\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # weight update\n",
    "        if ((idx + 1) % accumulation_steps == 0) or (idx + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "# loop over batches\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    \n",
    "    # get inputs\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # store\n",
    "    test_labels.extend(labels)\n",
    "    test_preds.extend(probabilities.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6bd3b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on a small subsample of the data:\n",
      "\n",
      "-- RESULTS --\n",
      "AUC: 0.64\n",
      "AP: 0.6978\n",
      "recall@100: 1.0\n",
      "CAP: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "preds = [label[1] for label in test_preds]\n",
    "evaluate(labels=test_labels, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c69ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
