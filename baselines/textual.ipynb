{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a5496c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import from main and experiments library\n",
    "import os\n",
    "from experiments_lib import *\n",
    "from roberta_lib import *\n",
    "os.chdir(\"../\")\n",
    "from library import *\n",
    "\n",
    "# filter the warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571421b",
   "metadata": {},
   "source": [
    "#### business failure prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f857102",
   "metadata": {},
   "source": [
    "We use the ECL benchmark dataset to predict next-year business failure from the multi-modal data contained in corporate 10K records. To this end, we use the following variables:\n",
    "\n",
    "- ```qualified```: \"Yes\" if the 10K record qualifies for inclusion in the LoPucki BRD, \"No\" if the 10K record does not qualify for inclusion in the LoPucki BRD and \"out-of-period\" if the 10K records was filed before 1993 or after 2021.\n",
    "- ```can_label```: \"True\" if we have all the necessary information to assign a label to the 10K record (```filing date``` and ```total asset value```), \"False\" otherwise\n",
    "- ```label```: \"True\" if the company filed for bankruptcy in the year following the filing date of the 10K, \"False\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1deacf",
   "metadata": {},
   "source": [
    "#### prepare data and pre-process text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4a90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and get subset\n",
    "dataset = pd.read_csv('ECL.csv', index_col=0)\n",
    "subset = dataset.loc[(dataset['can_label'] == True) & (dataset['qualified'] == 'Yes')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b17f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpera already exist\n"
     ]
    }
   ],
   "source": [
    "# path to corpera\n",
    "original_corpus = './data/original_corpus'\n",
    "clean_corpus = './data/clean_corpus'\n",
    "raw_corpus = './data/raw_corpus'\n",
    "\n",
    "# indicate of we still need to perform cleaning operations \n",
    "clean = True\n",
    "\n",
    "# Create directories\n",
    "try:\n",
    "    os.mkdir(clean_corpus + '/')\n",
    "    os.mkdir(raw_corpus + '/')\n",
    "    for i in range(1993,2024):\n",
    "        os.mkdir(clean_corpus + '/' + str(i))\n",
    "        os.mkdir(raw_corpus + '/' + str(i))\n",
    "except:\n",
    "    print('Corpera already exist')\n",
    "    clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524fea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# clean documents if indicated\n",
    "if clean:\n",
    "\n",
    "    # loop over documents\n",
    "    for idx, row in prediction_subset.iterrows():\n",
    "\n",
    "        # read file\n",
    "        filename = row['filename']\n",
    "        file_path = original_corpus + filename\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_data = json.load(f)\n",
    "\n",
    "        # extract relevant part and clean\n",
    "        document = file_data.get('item_7', '')\n",
    "        tokens = tokenize_lemmatize(document)\n",
    "        clean_tokens = remove_stop_punct_num(tokens)\n",
    "        clean_document = ' '.join(clean_tokens)\n",
    "\n",
    "        # create file paths\n",
    "        file_name_without_extension = os.path.splitext(filename)[0]\n",
    "        preprocessed_filepath = clean_corpus + file_name_without_extension + '.txt'\n",
    "        raw_filepath = raw_corpus + file_name_without_extension + '.txt'\n",
    "\n",
    "        # store\n",
    "        with open(preprocessed_filepath, \"w\", encoding=\"utf-8\") as preprocessed_file:\n",
    "            preprocessed_file.write(clean_document)\n",
    "\n",
    "        with open(raw_filepath, \"w\", encoding=\"utf-8\") as raw_file:\n",
    "            raw_file.write(document)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust file extension\n",
    "subset['filename'] = subset['filename'].str.replace('.json', '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a908ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "train = subset.loc[subset['bankruptcy_prediction_split'] == 'train']\n",
    "test = subset.loc[subset['bankruptcy_prediction_split'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1efe3e",
   "metadata": {},
   "source": [
    "## TF-IDF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc544895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split predictors and labels\n",
    "train_X = clean_corpus + train['filename']\n",
    "test_X = clean_corpus + test['filename']\n",
    "\n",
    "train_y = train['label']\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89052b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RESULTS --\n",
      "AUC: 0.8855\n",
      "AP: 0.2387\n",
      "recall@100: 0.2869\n",
      "CAP: 0.7711\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "TF_IDF = Pipeline([\n",
    "        ('vect', TfidfVectorizer(input='filename', lowercase=True, \n",
    "                                 strip_accents='ascii', stop_words='english', min_df=2, ngram_range = (1,2))),\n",
    "        ('clf', LogisticRegression(penalty = 'l1', C = 1, class_weight = 'balanced', \n",
    "                                   solver='liblinear'))])\n",
    "\n",
    "# train model\n",
    "TF_IDF.fit(X=train_X, y= train_y)\n",
    "\n",
    "# evaluate the model\n",
    "preds = TF_IDF.predict_proba(test_X)[:, 1]\n",
    "evaluate(labels=test_y, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf901673",
   "metadata": {},
   "source": [
    "## RoBERTa classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba92c8b",
   "metadata": {},
   "source": [
    "#### Initialize model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load pretrained RoBERTa tokenizer and model \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params - note that gradient accumulation is used to simulate larger batches\n",
    "batch_size = 16\n",
    "learning_rate = 2e-3\n",
    "num_epochs = 2\n",
    "accumulation_steps = 20\n",
    "\n",
    "# set optimiser and  weighted loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "class_counts = train['label'].value_counts().to_dict()\n",
    "total_samples = len(train)\n",
    "class_weights = [total_samples / (2 * class_counts[False]), total_samples / (2 * class_counts[True])]\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46fd7e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "train_dataset = CustomDataset(train, tokenizer, raw_corpus)\n",
    "test_dataset = CustomDataset(test, tokenizer, raw_corpus)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c12a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers in first epoch\n",
    "model.train()\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # unfreeze\n",
    "    if epoch == 1:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = True\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "    # loop over batches\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        # get inputs\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits \n",
    "\n",
    "        # backward pass\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # weight update\n",
    "        if ((idx + 1) % accumulation_steps == 0) or (idx + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "# loop over batches\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    \n",
    "    # get inputs\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # store\n",
    "    test_labels.extend(labels)\n",
    "    test_preds.extend(probabilities.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6bd3b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on a small subsample of the data:\n",
      "\n",
      "-- RESULTS --\n",
      "AUC: 0.64\n",
      "AP: 0.6978\n",
      "recall@100: 1.0\n",
      "CAP: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "preds = [label[1] for label in test_preds]\n",
    "evaluate(labels=test_labels, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c69ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
