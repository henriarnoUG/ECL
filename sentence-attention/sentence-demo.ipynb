{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a711d3",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfaad976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "from model_utils import *\n",
    "os.chdir(\"../\")\n",
    "from utils import *\n",
    "\n",
    "# filter the warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42dde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# specific imports \n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import random\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ae69",
   "metadata": {},
   "source": [
    "#### sentence-attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356a75e",
   "metadata": {},
   "source": [
    "In this notebook, we show the basic functionalities of the sentence-attention model on the ECL benchmark dataset. Note that we have stored the sentence embeddings and masks on disk for these experiments (see the ```embedding_demo.ipynb``` notebook for more information). Below, we show how to train and evaluate the model and how to display the sentences with the highest attention weights for a particular instance (we do this for a randomly sampled test instance from the top 50 instances with the highest probability of business failure assigned by the model from the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd259bb",
   "metadata": {},
   "source": [
    "#### prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba117b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path\n",
    "path_ECL = '../bankruptcy research data/ECL.csv' # change path to correct location\n",
    "path_CS = '../bankruptcy research data/Compustat/data.csv' # change path to correct location\n",
    "\n",
    "# read data and add financial features\n",
    "dataset = pd.read_csv(path_ECL, index_col=0)\n",
    "dataset = compustat_local(path_CS, dataset, update=False)\n",
    "dataset, predictors = compute_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34dffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test set\n",
    "subset = dataset.loc[(dataset['can_label'] == True) & (dataset['qualified'] == 'Yes')].reset_index(drop=True)\n",
    "train = subset.loc[subset['bankruptcy_prediction_split'] == 'train']\n",
    "test = subset.loc[subset['bankruptcy_prediction_split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7bbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean impute and normalize\n",
    "train_mean = train.loc[:, predictors].mean()\n",
    "train_std = train.loc[:, predictors].std()\n",
    "\n",
    "# Impute missing values with the mean\n",
    "train.loc[:, predictors] = train.loc[:, predictors].fillna(train_mean)\n",
    "test.loc[:, predictors] = test.loc[:, predictors].fillna(train_mean)\n",
    "\n",
    "# Normalize the data\n",
    "train.loc[:, predictors] = (train.loc[:, predictors] - train_mean) / train_std\n",
    "test.loc[:, predictors] = (test.loc[:, predictors] - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8a48e",
   "metadata": {},
   "source": [
    "#### set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2270c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seeds\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "seed = 1\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5f2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "negatives_batch = 4 \n",
    "batch_size = 320\n",
    "hidden_dim = 32\n",
    "training_time = 4 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f0b42-20a9-41e1-b6c2-e38c8640acbe",
   "metadata": {},
   "source": [
    "#### init datasets, model, optimiser and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd2c191-d204-4013-bbc3-0b6a8128bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom datasets\n",
    "train_dataset = SentenceDataset(dataframe=train)\n",
    "test_dataset = SentenceDataset(dataframe=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35918088-87df-4be8-a11f-4f00a116ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute current class distribution\n",
    "class_counts = train['label'].value_counts().to_dict()\n",
    "num_negatives = class_counts[False]\n",
    "num_positives = class_counts[True]\n",
    "\n",
    "# compute target class distribution\n",
    "target_frac_negatives = negatives_batch/(negatives_batch+1)\n",
    "target_frac_positives = 1 - target_frac_negatives\n",
    "\n",
    "# compute class weights for sampler\n",
    "class_weights = {False: (target_frac_negatives/num_negatives), True: (target_frac_positives/num_positives) }\n",
    "\n",
    "# compute the weight for each sample (as required for the WeightedRandomSampler)\n",
    "sample_weights = train['label'].map(class_weights).to_numpy()\n",
    "sample_weights = torch.Tensor(sample_weights).to(device)\n",
    "\n",
    "# init sampler\n",
    "set_seed(seed)\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94190aa4-f160-440e-9655-e02635bd66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ad5ae1-b09d-4e12-8e53-237715432ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init network, optimizer and loss\n",
    "network = SentenceAttentionNetwork(embedding_dim=384, feature_dim=28, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b2107-7a47-4b23-9a89-b985104232ce",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to train mode\n",
    "network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "for epoch in range(training_time):\n",
    "\n",
    "    # loop over batches\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "\n",
    "        ################# forward pass #################\n",
    "\n",
    "        # get input\n",
    "        embeddings = batch['sentence_embeddings'].to(device)\n",
    "        masks = batch['sentence_masks'].to(device)\n",
    "        features = batch['structured_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # get logits \n",
    "        logits, _ = network(embeddings, masks, features)\n",
    "\n",
    "        ################# backward pass #################\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(input=logits, target=labels)\n",
    "\n",
    "        # compute grads, update weights, reset grads\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993dc92-3193-4ebd-827a-91b138f343e2",
   "metadata": {},
   "source": [
    "#### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eec1fff-112e-4705-bd53-b93ab38c4a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceAttentionNetwork(\n",
       "  (linear_map): Linear(in_features=28, out_features=32, bias=True)\n",
       "  (key_layer): Linear(in_features=384, out_features=32, bias=True)\n",
       "  (value_layer): Linear(in_features=384, out_features=32, bias=True)\n",
       "  (classification): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to eval mode\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ecb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititalise list\n",
    "predictions = []\n",
    "attn_weights = []\n",
    "indices = []\n",
    "\n",
    "# loop over batches\n",
    "for idx, batch in enumerate(test_loader):\n",
    "\n",
    "    ################# get predictions #################\n",
    "\n",
    "    # get input\n",
    "    embeddings = batch['sentence_embeddings'].to(device)\n",
    "    masks = batch['sentence_masks'].to(device)\n",
    "    features = batch['structured_features'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    indx = batch['idx'].to(device)\n",
    "    \n",
    "    # get logits \n",
    "    with torch.no_grad():\n",
    "        logits, attn = network(embeddings, masks, features)\n",
    "    \n",
    "    # get predicted probabilities\n",
    "    preds = torch.sigmoid(logits)\n",
    "    preds = preds[:, 1]\n",
    "\n",
    "    ################# store predictions #################\n",
    "\n",
    "    # store\n",
    "    predictions.extend(preds.cpu().tolist())\n",
    "    indices.extend(indx.cpu().tolist())\n",
    "    attn_weights.append(attn.cpu().numpy())\n",
    "\n",
    "# stack attention weights\n",
    "attn_weights = np.vstack(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8292fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# set predictions\n",
    "test['predictions'] = None\n",
    "test.loc[indices, 'predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf31068-5bf7-4d14-80aa-310109bcbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "AUC, AP, recall, CAP = evaluate(test['label'], test['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813aee57-0d08-4cb6-ab79-b111b7e37169",
   "metadata": {},
   "source": [
    "#### sentence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 50 instances with higest probability of business failure\n",
    "top_pred = test.sort_values('predictions', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3947de1-bea4-41cb-807f-2c98ef0ef5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select a random instance\n",
    "row_id = np.random.randint(49)\n",
    "row = top_pred.iloc[row_id]\n",
    "idx = top_pred.index[row_id]\n",
    "\n",
    "# read text and tokenize\n",
    "with open(row['filename'], 'r') as f:\n",
    "    text = f.read()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# store attn_weights\n",
    "attn_weight = attn_weights[indices.index(idx)]\n",
    "\n",
    "# isolate attention on trainable vector\n",
    "attn_trainable = attn_weight[0]\n",
    "attn_sentences = attn_weight[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "478e6840-e0c8-4bbd-8fd6-51ae7ecefe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Sentence 1:\n",
      "Item 7.\n",
      "----------\n",
      "Sentence 2:\n",
      "North Dakota drilling activity declined approximately 65% during the year with the majority of activity returning to the core areas located in McKenzie, Mountrail, Williams and Dunn counties.\n",
      "----------\n",
      "Sentence 3:\n",
      "Operationally, 2015 proved to be a challenging year due to the downturn in commodity prices.\n",
      "----------\n",
      "Sentence 4:\n",
      "Historically, commodity prices have been volatile and we expect the volatility to continue in the future.\n",
      "----------\n",
      "Sentence 5:\n",
      "While rail transportation has historically been more expensive than pipeline transportation, Williston Basin prices have justified shipment by rail to markets such as St. James, Louisiana, which offers prices benchmarked to Brent/LLS.\n",
      "----------\n",
      "Sentence 6:\n",
      "North Dakota's average rig count has dropped from 171 on January 4, 2015 to 59 on December 31, 2015.\n",
      "----------\n",
      "Sentence 7:\n",
      "Additionally, our oil price differential to the NYMEX WTI benchmark price dropped from $13.67 per barrel in 2014 to $9.42 per barrel in 2015.\n",
      "----------\n",
      "Sentence 8:\n",
      "The following table lists average NYMEX prices for natural gas and oil for the years ended December 31, 2015, 2014 and 2013.\n",
      "----------\n",
      "Sentence 9:\n",
      "Our oil price differential to the NYMEX WTI benchmark price during 2015 was $9.42 per barrel, as compared to $13.67 per barrel in 2014.\n",
      "----------\n",
      "Sentence 10:\n",
      "Light sweet crude from the Williston Basin has a higher value at many major refining centers because of its higher quality relative to heavier and sour grades of oil; however, because of North Dakota's location relative to traditional oil transport centers, this higher value is generally offset to some extent by higher transportation costs.\n"
     ]
    }
   ],
   "source": [
    "# get top 10 sentences (highest attention weight)\n",
    "top_indices = np.argsort(attn_sentences)[-10:]\n",
    "top_indices_descending = top_indices[::-1]\n",
    "\n",
    "# print\n",
    "printed_sentences = set()\n",
    "for i, idx in enumerate(top_indices_descending):\n",
    "    sentence = sentences[idx]\n",
    "    if sentence not in printed_sentences:\n",
    "        print('-'*10)\n",
    "        print(f'Sentence {i+1}:')\n",
    "        print(sentence)\n",
    "        printed_sentences.add(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029efe03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
