{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a38751",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67ee863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "from model_utils import *\n",
    "os.chdir(\"../\")\n",
    "from utils import *\n",
    "\n",
    "# filter the warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecfbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# specific imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec589",
   "metadata": {},
   "source": [
    "#### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6336077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path\n",
    "path_ECL = '../bankruptcy research data/ECL.csv' # change path to correct location\n",
    "\n",
    "# read data and add financial features\n",
    "dataset = pd.read_csv(path_ECL, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa1c07",
   "metadata": {},
   "source": [
    "#### encode documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59886e01",
   "metadata": {},
   "source": [
    "In this notebook, we show how to encode a single document. For the experiments, we encoded each document and stored the embeddings and masks on disk. These then are read by the SentenceDataset for the SentenceAttentionNetwork. Adjust the paths accordingly in the model_utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3ba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_encode(text_path, sentence_encoder):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_path (str): path to text document (.txt)\n",
    "        sentence_encoder (sentence_transformer model): model from sentence_transformer library to encode sentences\n",
    "    Returns:\n",
    "        array with dense vector representations of the sentences in the document with shape (n_sentences, embedding_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    # read text file\n",
    "    with open(text_path, 'r', encoding=\"utf8\") as file:\n",
    "        text = file.read().strip()\n",
    "\n",
    "    # handle empty documents\n",
    "    if (text is None) or (text == ''):\n",
    "        text = \"not included\"\n",
    "\n",
    "    # tokenize document into sentences and encode\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = sentence_encoder.encode(sentences, batch_size=32)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e44fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_embeddings(embeddings, max_sentences):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embeddings (np Array): Array with dense vector representations of the sentences in the document with shape \n",
    "            (n_sentences, embedding_dim).\n",
    "        max_sentences (int): Maximum number of sentences that are encoded. If the document contains more sentences than \n",
    "            `max_sentences`, the embeddings are truncated to the first `max_sentences/2` and last `max_sentences/2` sentences. \n",
    "            If fewer sentences are present, the embeddings are padded with zero rows to reach `max_sentences` rows.\n",
    "    \n",
    "    Returns:\n",
    "        padded_embeddings (np Array): Array of shape (max_sentences, embedding_dim) containing the first `max_sentences/2`\n",
    "          and last `max_sentences/2` sentence embeddings if the document has more than `max_sentences` sentences. Otherwise, \n",
    "          the array contains all sentence embeddings from the document, padded with zero rows if necessary.\n",
    "        padding_mask (np Array): Array of shape (max_sentences,) where 0 indicates a row that contains an original sentence \n",
    "          embedding and 1 indicates a row that was added as padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get dimensions\n",
    "    n_sentences, embedding_dim = embeddings.shape\n",
    "\n",
    "    # long documents\n",
    "    if n_sentences > max_sentences:\n",
    "\n",
    "        # padded embeddings\n",
    "        half = max_sentences // 2\n",
    "        first_part = embeddings[:half]\n",
    "        last_part = embeddings[-half:]\n",
    "        padded_embeddings = np.vstack((first_part, last_part))\n",
    "\n",
    "        # padding mask\n",
    "        padding_mask = np.zeros(max_sentences)\n",
    "    \n",
    "    # short documents\n",
    "    else:\n",
    "\n",
    "        # padded embeddings\n",
    "        padded_embeddings = np.zeros((max_sentences, embedding_dim))\n",
    "        padded_embeddings[:n_sentences] = embeddings\n",
    "\n",
    "        # padding mask\n",
    "        padding_mask = np.zeros(max_sentences)\n",
    "        padding_mask[n_sentences:] = 1\n",
    "    \n",
    "    return padded_embeddings, padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6de59",
   "metadata": {},
   "source": [
    "#### apply to single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93bea999",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# init encoder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39da20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try code\n",
    "text_path = '../bankruptcy research data/raw_corpus' + dataset.iloc[100]['filename'].replace('.json', '.txt')\n",
    "embeddings = doc_encode(text_path, sentence_encoder)\n",
    "padded, mask = pad_embeddings(embeddings, max_sentences=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a110da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 384)\n"
     ]
    }
   ],
   "source": [
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db807f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b04d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0787237   0.0460993   0.0035307  ... -0.01979921 -0.10768701\n",
      "   0.01217997]\n",
      " [-0.0165556   0.03531491 -0.04758731 ... -0.06410385 -0.02580261\n",
      "  -0.00225563]\n",
      " [-0.04243159  0.01894433  0.03861635 ...  0.07538494  0.04574082\n",
      "   0.01861062]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95fe591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e968d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
